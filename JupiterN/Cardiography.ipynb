{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Basic functions for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def drop_line_if_it_contains(data_csv: DataFrame, column: str, value):\n",
    "    for i in range(len(data_csv[column])):\n",
    "        if data_csv[column].at[i] == value:\n",
    "            data_csv.drop([i], axis=0, inplace=True)\n",
    "\n",
    "    return data_csv\n",
    "\n",
    "# replacing 0 to NaN in the dataframe\n",
    "def null_to_NaN(X, except_list):\n",
    "    cols_with_missing_val = detect_vals(X, 0, except_list).index[0]\n",
    "    X = replace_val_in_cols(X, [cols_with_missing_val], 0, np.nan)\n",
    "    return X\n",
    "\n",
    "# Counting an amount of null values in dataset by column\n",
    "def detect_vals(obj_to_describe, val, exclude_col=[]):\n",
    "    # missing values by columns\n",
    "    obj_to_describe = obj_to_describe.drop(exclude_col, axis=1)\n",
    "    missing_val_count_by_column = (obj_to_describe.isin([val]).sum())\n",
    "    return (missing_val_count_by_column)\n",
    "\n",
    "def read_csv(name):\n",
    "    return pd.read_csv('../DataSource/' + name, encoding='utf-8')\n",
    "\n",
    "# Replaces each val in appropriate cols\n",
    "def replace_val_in_cols(data_csv: DataFrame, column_list, old_value: str, new_value: str):\n",
    "    for col in column_list:\n",
    "        data_csv[col] = data_csv[col].replace(old_value, new_value)\n",
    "    return data_csv\n",
    "\n",
    "def replace_val_in_cols_except(data_csv: DataFrame, column: str, except_values: [], new_value: str):\n",
    "    for i in range(len(data_csv[column])):\n",
    "        if any(x == data_csv[column].at[i] for x in except_values):\n",
    "            continue\n",
    "        data_csv[column].at[i] = new_value\n",
    "\n",
    "    return data_csv\n",
    "\n",
    "# replacing inf with nan\n",
    "def replace_inf_with_nan_and_impute(X:DataFrame):\n",
    "    df = apply_encoder(X)\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df = simple_imputing_data(df, df)\n",
    "    return df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Functions for imputing and encoding non numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Replacing missing values (imputing) according to certain strategy\n",
    "def simple_imputing_data(X_train, X_valid, strategy:str = 'mean'):\n",
    "    simple_imputer = SimpleImputer(missing_values=np.nan, strategy=strategy)\n",
    "    imputed_X_train = pd.DataFrame(simple_imputer.fit_transform(X_train))\n",
    "    imputed_X_valid = pd.DataFrame(simple_imputer.transform(X_valid))\n",
    "    # Imputation removed column names; put them back\n",
    "    imputed_X_train.columns = X_train.columns\n",
    "    imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "    return imputed_X_train, imputed_X_valid\n",
    "\n",
    "# Encoding labels\n",
    "def apply_encoder(X: DataFrame):\n",
    "    s = (X.dtypes == 'object')\n",
    "    object_cols = list(s[s].index)\n",
    "\n",
    "    for col in object_cols:\n",
    "        X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Enumerating initial files(DataSource folder) and saving it as single file inlined_ecgs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_ecg(ECGS_filename):\n",
    "    source_path = '../DataSource/'\n",
    "    directory = source_path + 'metrics/'\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(f):\n",
    "            if 'df' not in locals():\n",
    "                df = read_csv(f)\n",
    "                df_inlined = inline_single_ecg(df)\n",
    "            else:\n",
    "                current_line = read_csv(f)\n",
    "                df = pd.concat([df, current_line])\n",
    "                df_inlined = pd.concat([df_inlined, inline_single_ecg(current_line)])\n",
    "\n",
    "    df.to_csv('../DataSource/' + ECGS_filename)\n",
    "    df_inlined.to_csv('../DataSource/' + 'inlined_'+ECGS_filename)\n",
    "    return df_inlined\n",
    "\n",
    "def inline_single_ecg(df: DataFrame):\n",
    "    patient_id = df.iloc[0, 0]\n",
    "    coloumns_to_drop = ['patient_id', 'ecg_id', 'lead']\n",
    "    for col in coloumns_to_drop:\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "    df.index = df.index + 1\n",
    "    df_out = df.stack()\n",
    "    df_out.index = df_out.index.map('{0[1]}_{0[0]}'.format)\n",
    "    df_inlined = df_out.to_frame().T\n",
    "    df_inlined['patient_id'] = patient_id\n",
    "    return df_inlined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Casting to binary task replacing:\n",
    "    MI as false(0);\n",
    "    NORM as true(1);\n",
    "    other values are dropped out from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_dataset_core(name_csv: str):\n",
    "    df = read_csv('../DataSource/' + name_csv)\n",
    "    col = 'diagnostic_superclass'\n",
    "    replace_val_in_cols(df, [col], \"['MI']\", 'False')\n",
    "    replace_val_in_cols(df, [col], \"['NORM']\", 'True')\n",
    "\n",
    "    replace_val_in_cols_except(df, col, ['True', 'False'], col)\n",
    "    drop_line_if_it_contains(df, col, col)\n",
    "\n",
    "    return apply_encoder(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Building Random Forest Model and measuring it with confusion matrix, accuracy(jaccard_score) and MAE(Mean absolute error regression loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "def build_confusion_matrix(predictions, y_test):\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "from random import uniform\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def rfc(X_train, X_valid, y_train, y_valid, random_state=round(uniform(0, 300))):\n",
    "    reg_model = RandomForestClassifier(n_estimators=10000, random_state=random_state, min_samples_split = 2,\n",
    "                                       max_features='sqrt', criterion='gini', n_jobs=-1)\n",
    "    reg_model = reg_model.fit(X_train, y_train)\n",
    "    predicted = reg_model.predict(X_valid)\n",
    "    predictions = [round(value) for value in predicted]\n",
    "    accuracy = accuracy_score(y_valid, predictions)\n",
    "    build_confusion_matrix(predicted, y_valid)\n",
    "    print(\"Accuracy: %.3f%%\" % (accuracy * 100.0))\n",
    "    print(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "def estimate(X_train, X_valid, y_train, y_valid, random_state=round(uniform(0, 300))):\n",
    "    names = [\n",
    "        \"Nearest Neighbors\",\n",
    "        \"RBF SVM\",\n",
    "        \"Decision Tree\",\n",
    "        \"Random Forest\",\n",
    "        \"Neural Net\",\n",
    "    ]\n",
    "\n",
    "    classifiers = [\n",
    "        KNeighborsClassifier(3),\n",
    "        SVC(gamma=2, C=1),\n",
    "        DecisionTreeClassifier(random_state=random_state, max_features='log2'),\n",
    "        RandomForestClassifier(random_state=random_state, min_samples_split=20,\n",
    "                               max_features='log2', criterion='entropy'),\n",
    "        MLPClassifier(alpha=1, max_iter=1000)\n",
    "    ]\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_valid)\n",
    "        predictions = [round(value) for value in predictions]\n",
    "        accuracy = accuracy_score(y_valid, predictions)\n",
    "        print(name +\" :Accuracy: %.3f%%\" % (accuracy * 100.0))\n",
    "        print(name + \" :Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))\n",
    "\n",
    "\n",
    "# Splitting to test and train data (82/1)\n",
    "def prepare_test_data(X_full: DataFrame, beside_list=[]):\n",
    "    target_column = 'diagnostic_superclass'\n",
    "    randomState = round(uniform(0, 300))\n",
    "    y = X_full[target_column]\n",
    "    X = null_to_NaN(X_full.drop([target_column], axis=1), beside_list)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.82, test_size=0.18, random_state=randomState)\n",
    "\n",
    "    return (X_train, X_valid, y_train, y_valid, randomState)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function. Uncomment line with \"prepare_ecg\" function only if there is no\n",
    "joined_data.csv in the DataSource folder(enumerating files might take up to 5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    patients_info_filename = 'patientsInfo.csv'\n",
    "    ECGS_filename = 'ecgs.csv'\n",
    "    path = '../DataSource/'\n",
    "\n",
    "    # print('converting initial data to single inlined data')\n",
    "    # # converts initial data to single inlined data/(5min)\n",
    "    # # run only once if inlined_ecgs.csv was not generated\n",
    "    # prepare_ecg(ECGS_filename)\n",
    "\n",
    "    # Joining ECGS_data and patients info\n",
    "    ECGS_data = read_csv('../DataSource/' + 'inlined_' + ECGS_filename)\n",
    "\n",
    "    print('Preparing generated DF(values replacing, imputing)')\n",
    "    dataframe = prepare_dataset_core(patients_info_filename)\n",
    "    X = dataframe.set_index('patient_id').join(ECGS_data.set_index('patient_id'))\n",
    "    X.to_csv(path + \"joined_data.csv\")\n",
    "    X = replace_inf_with_nan_and_impute(X)\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid, randomState = prepare_test_data(X)\n",
    "\n",
    "    print('Models fit comparison')\n",
    "    estimate(X_train, X_valid, y_train, y_valid, randomState)\n",
    "\n",
    "    # Execution for optimized random forest model\n",
    "    print('Building ML model')\n",
    "    rfc(X_train, X_valid, y_train, y_valid, randomState)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Executing pipeline\n",
    "Train/test ratio: 82/18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing generated DF(values replacing, imputing)\n",
      "Models fit comparison\n",
      "Nearest Neighbors :Accuracy: 68.286%\n",
      "Nearest Neighbors :Mean Absolute Error: 0.3171364985163205\n",
      "RBF SVM :Accuracy: 73.887%\n",
      "RBF SVM :Mean Absolute Error: 0.26112759643916916\n",
      "Decision Tree :Accuracy: 83.680%\n",
      "Decision Tree :Mean Absolute Error: 0.1632047477744807\n",
      "Random Forest :Accuracy: 96.105%\n",
      "Random Forest :Mean Absolute Error: 0.03894658753709199\n",
      "Neural Net :Accuracy: 54.303%\n",
      "Neural Net :Mean Absolute Error: 0.456973293768546\n",
      "Building ML model\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
